import nibabel as nib
import numpy as np
import os
from typing import Dict, Optional


def analyze_dataset_automatically(predictions_path: str, 
                                ground_truth_path: Optional[str] = None,
                                pred_pattern: str = 'pred_s', 
                                gt_pattern: str = 'label_annot_',
                                is_pred_file_specified: bool = False) -> Dict:
    """
    è‡ªåŠ¨åˆ†ææ•°æ®é›†ï¼Œæå–æ‰€æœ‰å¿…è¦çš„å‚æ•°
    
    Args:
        predictions_path: é¢„æµ‹æ–‡ä»¶è·¯å¾„æˆ–ç›®å½•è·¯å¾„
        ground_truth_path: æ ‡æ³¨æ–‡ä»¶è·¯å¾„æˆ–ç›®å½•è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨predictions_pathï¼‰
        pred_pattern: é¢„æµ‹æ–‡ä»¶æ¨¡å¼
        gt_pattern: æ ‡æ³¨æ–‡ä»¶æ¨¡å¼
        
    Returns:
        åŒ…å«æ‰€æœ‰è‡ªåŠ¨æ£€æµ‹å‚æ•°çš„å­—å…¸
    """
    
    # æ™ºèƒ½å¤„ç†æ–‡ä»¶è·¯å¾„ vs ç›®å½•è·¯å¾„
    specified_pred_file = None
    if os.path.isfile(predictions_path):
        # å¦‚æœæ˜¯æ–‡ä»¶è·¯å¾„ï¼Œæå–ç›®å½•å’Œæ–‡ä»¶å
        pred_dir = os.path.dirname(predictions_path)
        # å¦‚æœdirnameè¿”å›ç©ºå­—ç¬¦ä¸²ï¼Œä½¿ç”¨å½“å‰ç›®å½•
        if not pred_dir:
            pred_dir = '.'
        specified_pred_file = os.path.basename(predictions_path)
        print(f"ğŸ” æ£€æµ‹åˆ°é¢„æµ‹æ–‡ä»¶: {specified_pred_file}")
        
        # ç”¨æˆ·æŒ‡å®šäº†å…·ä½“é¢„æµ‹æ–‡ä»¶ï¼Œéœ€è¦ä»æ–‡ä»¶åæ¨æ–­pattern
        is_pred_file_specified = True
        
        # ä»é¢„æµ‹æ–‡ä»¶åæ¨æ–­pattern
        # å»é™¤.nii.gzåç¼€ï¼Œç„¶åå»é™¤æ•°å­—åç¼€æ¥å¾—åˆ°pattern
        base_name = specified_pred_file.replace('.nii.gz', '')
        
        # å°è¯•å‡ ç§å¸¸è§çš„patternæ¨æ–­æ–¹å¼
        if pred_pattern not in base_name:
            # å¦‚æœæ–‡ä»¶åä¸åŒ…å«é»˜è®¤patternï¼Œå°è¯•æ¨æ–­
            import re
            # å…ˆå°è¯•å»é™¤ç»“å°¾çš„æ•°å­—å¾—åˆ°pattern
            pattern_match = re.match(r'(.+?)(_\d+)?$', base_name)
            if pattern_match:
                inferred_pattern = pattern_match.group(1)
                # å¦‚æœæ¨æ–­çš„patternåˆç†ï¼ˆä¸ä¸ºç©ºä¸”åŒ…å«å­—ç¬¦ï¼‰ï¼Œä½¿ç”¨å®ƒ
                if inferred_pattern and len(inferred_pattern) > 2:
                    pred_pattern = inferred_pattern
                    print(f"ğŸ”„ ä»æ–‡ä»¶åæ¨æ–­é¢„æµ‹patternä¸º: {pred_pattern}")
                else:
                    # å¦‚æœæ²¡æœ‰æ•°å­—åç¼€ï¼Œä½¿ç”¨æ•´ä¸ªæ–‡ä»¶åä½œä¸ºpattern
                    pred_pattern = base_name
                    print(f"ğŸ”„ ä½¿ç”¨å®Œæ•´æ–‡ä»¶åä½œä¸ºé¢„æµ‹pattern: {pred_pattern}")
        
        predictions_path = pred_dir
    else:
        pred_dir = predictions_path
    
    # å¤„ç†ground_truth_path
    if ground_truth_path is None:
        ground_truth_path = pred_dir
        # å½“ç”¨æˆ·æŒ‡å®šå…·ä½“é¢„æµ‹æ–‡ä»¶æ—¶ï¼Œä¸è°ƒæ•´gt_patternï¼Œä¿æŒé»˜è®¤çš„label_annot_
        # ä¸éœ€è¦æ‰“å°ï¼Œå› ä¸ºè¿˜æ²¡æœ‰å‘ç°æ ‡æ³¨æ–‡ä»¶
    elif os.path.isfile(ground_truth_path):
        gt_dir = os.path.dirname(ground_truth_path)
        gt_filename = os.path.basename(ground_truth_path)
        print(f"ğŸ” æ£€æµ‹åˆ°æ ‡æ³¨æ–‡ä»¶: {gt_filename}")
        
        # åªæœ‰å½“ç”¨æˆ·æ˜ç¡®æŒ‡å®šæ ‡æ³¨æ–‡ä»¶æ—¶ï¼Œæ‰è°ƒæ•´gt_pattern
        if not gt_pattern in gt_filename:
            for possible_pattern in ['label_annot_', 'pred_annot_', 'label_', 'gt_', 'annotation_']:
                if possible_pattern in gt_filename:
                    gt_pattern = possible_pattern
                    print(f"ğŸ”„ è‡ªåŠ¨è°ƒæ•´æ ‡æ³¨patternä¸º: {gt_pattern}")
                    break
        
        ground_truth_path = gt_dir
    else:
        # ground_truth_pathæ˜¯ç›®å½•ï¼Œä¿æŒgt_patternä¸å˜
        pass
    
    # 1. å‘ç°æ‰€æœ‰æ–‡ä»¶
    try:
        if is_pred_file_specified and specified_pred_file:
            # ç”¨æˆ·æŒ‡å®šäº†å…·ä½“é¢„æµ‹æ–‡ä»¶ï¼Œåªä½¿ç”¨è¯¥æ–‡ä»¶
            pred_files = [specified_pred_file]
        else:
            # ç”¨patternæœç´¢é¢„æµ‹æ–‡ä»¶
            all_pred_files = os.listdir(predictions_path)
            pred_files = sorted([f for f in all_pred_files if pred_pattern in f and f.endswith('.nii.gz')])
        
        # æ€»æ˜¯ç”¨gt_patternæœç´¢æ ‡æ³¨æ–‡ä»¶
        if ground_truth_path == predictions_path:
            all_files = os.listdir(predictions_path)
            gt_files = sorted([f for f in all_files if gt_pattern in f and f.endswith('.nii.gz')])
        else:
            all_gt_files = os.listdir(ground_truth_path)
            gt_files = sorted([f for f in all_gt_files if gt_pattern in f and f.endswith('.nii.gz')])
    except Exception as e:
        raise ValueError(f"æ— æ³•è¯»å–ç›®å½• {predictions_path}: {e}")
    
    print(f'ğŸ” è‡ªåŠ¨å‘ç°: {len(pred_files)} ä¸ªé¢„æµ‹æ–‡ä»¶, {len(gt_files)} ä¸ªæ ‡æ³¨æ–‡ä»¶')
    
    # 2. åˆ†ææ ‡æ³¨æ–‡ä»¶ä»¥ç¡®å®šæ•°æ®ç‰¹å¾
    all_classes = set()
    class_counts = {}
    total_pixels = 0
    file_shapes = []
    data_types = set()
    
    print('ğŸ“Š åˆ†ææ ‡æ³¨æ–‡ä»¶ç‰¹å¾...')
    for gt_file in gt_files:
        file_path = os.path.join(ground_truth_path, gt_file)
        if os.path.exists(file_path):
            data = nib.load(file_path).get_fdata()
            file_shapes.append(data.shape)
            data_types.add(str(data.dtype))
            
            # ç»Ÿè®¡ç±»åˆ«
            unique_classes = np.unique(data)
            unique_classes = unique_classes[~np.isnan(unique_classes)]
            
            for cls in unique_classes:
                cls_int = int(cls)
                all_classes.add(cls_int)
                count = np.sum(data == cls)
                class_counts[cls_int] = class_counts.get(cls_int, 0) + count
                
            total_pixels += data.size
    
    # 3. è‡ªåŠ¨æ¨æ–­å‚æ•°
    sorted_classes = sorted(list(all_classes))
    max_class = max(sorted_classes) if sorted_classes else 0
    
    # æ™ºèƒ½ç¡®å®šnum_classes
    if len(sorted_classes) <= 2:
        # äºŒè¿›åˆ¶æƒ…å†µ
        num_classes = 2
        is_binary = True
    else:
        # å¤šç±»åˆ«æƒ…å†µï¼šä½¿ç”¨max_class + 1
        num_classes = max_class + 1
        is_binary = False
    
    auto_params = {
        'all_classes': sorted_classes,
        'num_classes': num_classes,
        'max_class': max_class,
        'is_binary': is_binary,
        'is_multiclass': not is_binary,
        'has_background': 0 in all_classes,
        'class_distribution': class_counts,
        'total_pixels': total_pixels,
        'file_shapes': file_shapes,
        'data_types': list(data_types),
        'num_gt_files': len(gt_files),
        'num_pred_files': len(pred_files),
        'detected_pred_pattern': pred_pattern,
        'detected_gt_pattern': gt_pattern
    }
    
    # 4. æ™ºèƒ½å»ºè®®èƒŒæ™¯å¤„ç†
    if auto_params['has_background']:
        background_ratio = class_counts.get(0, 0) / total_pixels if total_pixels > 0 else 0
        auto_params['background_ratio'] = background_ratio
        # å¦‚æœèƒŒæ™¯å æ¯”è¶…è¿‡50%ï¼Œå»ºè®®æ’é™¤èƒŒæ™¯
        auto_params['exclude_background_recommended'] = background_ratio > 0.5
    else:
        auto_params['background_ratio'] = 0.0
        auto_params['exclude_background_recommended'] = False
    
    # 5. éªŒè¯predictionså…¼å®¹æ€§
    if pred_files:
        print('ğŸ”§ æ£€æŸ¥é¢„æµ‹æ–‡ä»¶å…¼å®¹æ€§...')
        pred_classes = set()
        pred_shapes = []
        sample_count = min(3, len(pred_files))  # æ£€æŸ¥å‰3ä¸ªæ–‡ä»¶
        
        for pred_file in pred_files[:sample_count]:
            file_path = os.path.join(predictions_path, pred_file)
            if os.path.exists(file_path):
                data = nib.load(file_path).get_fdata()
                pred_shapes.append(data.shape)
                unique_classes = np.unique(data)
                unique_classes = unique_classes[~np.isnan(unique_classes)]
                for cls in unique_classes:
                    pred_classes.add(int(cls))
                    
        auto_params['pred_classes'] = sorted(list(pred_classes))
        auto_params['pred_shapes'] = pred_shapes
        auto_params['pred_label_compatible'] = pred_classes.issubset(all_classes)
        auto_params['shapes_consistent'] = len(set(str(s) for s in file_shapes + pred_shapes)) == 1
        
        # æ£€æŸ¥é¢„æµ‹æ˜¯å¦å¯èƒ½æ˜¯æ¦‚ç‡æ ¼å¼
        if pred_files and len(pred_classes) > 0:
            max_pred_val = max(pred_classes) if pred_classes else 0
            min_pred_val = min(pred_classes) if pred_classes else 0
            auto_params['pred_seems_probabilities'] = (min_pred_val >= 0 and max_pred_val <= 1 and 
                                                     len(pred_classes) > 10)
        else:
            auto_params['pred_seems_probabilities'] = False
    
    return auto_params


def print_analysis_summary(params: Dict) -> None:
    """æ‰“å°åˆ†æç»“æœæ‘˜è¦"""
    print('\n' + '='*60)
    print('ğŸ“‹ è‡ªåŠ¨æ•°æ®é›†åˆ†æç»“æœ')
    print('='*60)
    
    print(f"ğŸ“ æ–‡ä»¶æ•°é‡: {params['num_pred_files']} é¢„æµ‹, {params['num_gt_files']} æ ‡æ³¨")
    print(f"ğŸ·ï¸  æ£€æµ‹åˆ°ç±»åˆ«: {params['all_classes']}")
    print(f"ğŸ“Š å®é™…ç±»åˆ«æ•°é‡: {len(params['all_classes'])} ä¸ªç±»åˆ«")
    
    if params['has_background']:
        bg_pct = params['background_ratio'] * 100
        print(f"ğŸ¯ èƒŒæ™¯ç±»å æ¯”: {bg_pct:.1f}%")
    
    # è§£é‡Šnum_classesçš„è®¡ç®—é€»è¾‘
    max_class = max(params['all_classes']) if params['all_classes'] else 0
    if len(params['all_classes']) != params['num_classes']:
        print(f"ğŸ”¢ æ¨è num_classes: {params['num_classes']} (max_class + 1 = {max_class} + 1, é€‚ç”¨äºç¨€ç–ç±»åˆ«)")
    else:
        print(f"ğŸ”¢ æ¨è num_classes: {params['num_classes']}")
    
    if params['is_binary']:
        print("ğŸ“‹ æ¨èæ¨¡å¼: äºŒè¿›åˆ¶åˆ†å‰²")
    else:
        print("ğŸ“‹ æ¨èæ¨¡å¼: å¤šç±»åˆ«åˆ†å‰²")
    
    if params.get('pred_label_compatible', True):
        print("âœ… é¢„æµ‹ä¸æ ‡æ³¨å…¼å®¹")
    else:
        print("âš ï¸  é¢„æµ‹ä¸æ ‡æ³¨å¯èƒ½ä¸å…¼å®¹")
        print(f"   é¢„æµ‹ç±»åˆ«: {params.get('pred_classes', [])}")
        print(f"   æ ‡æ³¨ç±»åˆ«: {params['all_classes']}")
    
    print('\nğŸš€ å»ºè®®çš„å‘½ä»¤è¡Œå‚æ•°:')
    if params['is_binary']:
        cmd = "# äºŒè¿›åˆ¶æ¨¡å¼ (æ— éœ€é¢å¤–å‚æ•°)"
    else:
        cmd = f"--multiclass --num_classes {params['num_classes']}"
        
    if params.get('exclude_background_recommended', False):
        cmd += "  # å»ºè®®æ’é™¤èƒŒæ™¯ç±»"
    else:
        cmd += " --include_background"
    
    print(f"   {cmd}")


def get_auto_config(predictions_path: str,
                   ground_truth_path: Optional[str] = None,
                   pred_pattern: str = 'pred_s',
                   gt_pattern: str = 'label_annot_',
                   verbose: bool = True) -> Dict:
    """
    è·å–è‡ªåŠ¨é…ç½®çš„ä¾¿æ·å‡½æ•°
    
    Returns:
        è‡ªåŠ¨æ£€æµ‹çš„é…ç½®å‚æ•°ï¼Œå¯ç›´æ¥ç”¨äºè¯„ä¼°è„šæœ¬
    """
    # æ£€æµ‹æ˜¯å¦æŒ‡å®šäº†å…·ä½“æ–‡ä»¶
    is_pred_file_specified = os.path.isfile(predictions_path)
    
    params = analyze_dataset_automatically(predictions_path, ground_truth_path, 
                                         pred_pattern, gt_pattern, is_pred_file_specified)
    
    if verbose:
        print_analysis_summary(params)
    
    # è¿”å›å¯ç›´æ¥ä½¿ç”¨çš„é…ç½®
    config = {
        'multiclass': params['is_multiclass'],
        'num_classes': params['num_classes'],
        'exclude_background': params.get('exclude_background_recommended', True),
        'pred_pattern': params['detected_pred_pattern'],
        'gt_pattern': params['detected_gt_pattern']
    }
    
    return config, params


if __name__ == '__main__':
    # æµ‹è¯•è‡ªåŠ¨é…ç½®åŠŸèƒ½
    config, details = get_auto_config('.', '.', 'pred_annot_', 'label_annot_')
    
    print(f"\nğŸ¯ æœ€ç»ˆé…ç½®: {config}")